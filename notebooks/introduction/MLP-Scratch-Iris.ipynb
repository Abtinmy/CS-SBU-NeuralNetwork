{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8711f36",
   "metadata": {},
   "source": [
    "# Implementing a Multi-Layer Perceptron from Scratch\n",
    "**Author** : Mobin Nesari\n",
    "\n",
    "**Prepared for** : The Artificial Neural Network Graduate course 2023 Shahid Beheshti University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef7499",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network that is commonly used for supervised learning tasks, such as binary and multi-class classification. In this notebook, we'll implement an MLP from scratch using only Numpy and implement it to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821ca41",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "\n",
    "We will use the well-known Iris dataset for this demonstration. The Iris dataset contains 150 instances, where each instance has 4 features and a binary label indicating the species of the Iris flower. Our goal is to train an MLP to correctly classify the species based on the 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471247d5",
   "metadata": {},
   "source": [
    "## Introduction to target dataset\n",
    "The Iris dataset is a widely used dataset in machine learning, it contains information about different species of iris flowers. The dataset contains 150 samples of iris flowers with 4 features for each flower: **sepal length**, **sepal width**, **petal length**, and **petal width**. The goal of using this dataset is to train a machine learning model that can accurately predict the species of a given iris flower based on its features.\n",
    "\n",
    "### Features Used in the Jupyter Notebook\n",
    "In this Jupyter notebook, we will use the **petal length** and **petal width** features of the Iris dataset to train and evaluate our Multi-Layer Perceptron (MLP). These two features were selected because they provide a good representation of the variations in the different species of iris flowers, and they are also easy to visualize.\n",
    "\n",
    "By using only 2 features, we were able to create a two-dimensional scatter plot of the data, which makes it easier to understand the relationships between the features and the target variable (species). The MLP was then trained on this data, and the accuracy of the model was evaluated based on its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c4b2d",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start by importing the required libraries and loading the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b6ba6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0\n",
    "y = y.reshape([150,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26f61e",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "Before we build our MLP, let's define the activation functions that we'll be using. For this demonstration, we'll be using the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a656fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4224e6e",
   "metadata": {},
   "source": [
    "## MLP Class Definition\n",
    "Now, let's define the MLP class. The MLP class has 3 methods:\n",
    "- `__init__` method that initializes the weights and biases of the MLP\n",
    "- `fit` method that trains the MLP on the training data\n",
    "- `predict` method that uses the trained MLP to make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ffc732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights randomly\n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # initialize biases to 0\n",
    "        self.bias1 = np.zeros((1, self.hidden_size))\n",
    "        self.bias2 = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            # feedforward\n",
    "            layer1 = X.dot(self.weights1) + self.bias1\n",
    "            activation1 = sigmoid(layer1)\n",
    "            layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "            activation2 = sigmoid(layer2)\n",
    "            \n",
    "            # backpropagation\n",
    "            error = activation2 - y\n",
    "            d_weights2 = activation1.T.dot(error * sigmoid_derivative(layer2))\n",
    "            d_bias2 = np.sum(error * sigmoid_derivative(layer2), axis=0, keepdims=True)\n",
    "            error_hidden = error.dot(self.weights2.T) * sigmoid_derivative(layer1)\n",
    "            d_weights1 = X.T.dot(error_hidden)\n",
    "            d_bias1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "            \n",
    "            # update weights and biases\n",
    "            self.weights2 -= self.learning_rate * d_weights2\n",
    "            self.bias2 -= self.learning_rate * d_bias2\n",
    "            self.weights1 -= self.learning_rate * d_weights1\n",
    "            self.bias1 -= self.learning_rate * d_bias1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        layer1 = X.dot(self.weights1) + self.bias1\n",
    "        activation1 = sigmoid(layer1)\n",
    "        layer2 = activation1.dot(self.weights2) + self.bias2\n",
    "        activation2 = sigmoid(layer2)\n",
    "        return (activation2 > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4c58b",
   "metadata": {},
   "source": [
    "This code defines a class called `MLP` that implements a Multi-Layer Perceptron (MLP) algorithm. The MLP is a type of artificial neural network that can be used for classification and regression problems.\n",
    "\n",
    "The class takes 4 parameters in its constructor method: input_size, hidden_size, output_size, and learning_rate. input_size is the number of input features, hidden_size is the number of neurons in the hidden layer, output_size is the number of output classes, and learning_rate is the learning rate used in the training process.\n",
    "\n",
    "The class also has two methods: fit and predict. The fit method trains the MLP on the provided data and target values, and the predict method makes predictions for new data based on the trained MLP.\n",
    "\n",
    "In the `fit` method, the MLP uses a feedforward and backpropagation algorithm to learn the weights and biases that minimize the prediction error. The feedforward part calculates the activations of the hidden and output layers, while the backpropagation part updates the weights and biases based on the gradient of the prediction error. The training process repeats for a specified number of epochs until convergence or until the maximum number of epochs is reached.\n",
    "\n",
    "In the `predict` method, the MLP applies the trained weights and biases to new data to make predictions, and returns the predictions as binary values.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6f4b3",
   "metadata": {},
   "source": [
    "## Training and Evaluating the MLP\n",
    "Let's now train and evaluate the MLP on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08f4be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the MLP class\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# train the MLP on the training data\n",
    "mlp.fit(X, y)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "# evaluate the accuracy of the MLP\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df506f3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we implemented a Multi-Layer Perceptron (MLP) from scratch using only Numpy. We trained the MLP on the Iris dataset and achieved an accuracy of around 90%. This demonstration serves as a good starting point for understanding the fundamentals of MLPs and building more complex neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
